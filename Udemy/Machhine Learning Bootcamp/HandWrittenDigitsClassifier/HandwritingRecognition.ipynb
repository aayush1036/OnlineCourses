{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(888)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jain_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os \r\n",
    "import numpy as np\r\n",
    "import tensorflow.compat.v1 as tf\r\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import strftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TRAIN_PATH = 'MNIST/digit_xtrain.csv'\r\n",
    "X_TEST_PATH = 'MNIST/digit_xtest.csv'\r\n",
    "Y_TRAIN_PATH = 'MNIST/digit_ytrain.csv'\r\n",
    "Y_TEST_PATH = 'MNIST/digit_ytest.csv'\r\n",
    "\r\n",
    "LOGGING_PATH = 'tensorboard_mnist_digit_logs/'\r\n",
    "\r\n",
    "NR_CLASSES = 10\r\n",
    "VALIDATION_SIZE = 10000\r\n",
    "\r\n",
    "IMAGE_WIDTH = 28\r\n",
    "IMAGE_HEIGHT = 28\r\n",
    "COLOR_CHANNELS = 1\r\n",
    "TOTAL_INPUTS = IMAGE_HEIGHT*IMAGE_WIDTH*COLOR_CHANNELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 311 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "y_train_all = np.loadtxt(Y_TRAIN_PATH, delimiter=',',dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.loadtxt(Y_TEST_PATH, delimiter=',',dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_all = np.loadtxt(X_TRAIN_PATH,delimiter=',',dtype=int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "x_test = np.loadtxt(X_TEST_PATH, delimiter=',',dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale \n",
    "x_train_all, x_test = x_train_all/255.0, x_test/255.0 #dividing by 255.0 to scale the outputs and to convert the data into a float\n",
    "#converting the data to a float helps us to ease the calculation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert target values to One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_all = np.eye(NR_CLASSES)[y_train_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.eye(NR_CLASSES)[y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create validation dataset from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train_all[:VALIDATION_SIZE]\n",
    "y_val = y_train_all[:VALIDATION_SIZE]\n",
    "x_train = x_train_all[VALIDATION_SIZE:]\n",
    "y_train = y_train_all[VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are a generalization of arrays to a higher dimensions <br>\n",
    "Tensors follow rules of linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Tensorflow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, TOTAL_INPUTS],name='X')\r\n",
    "# we put none in the shape because the first dimension will hold the number of samples that we pass and we can change the batch size later on while training and we put 784 in the shape because we have 784 features in an image \r\n",
    "Y = tf.placeholder(tf.float32, shape=[None, NR_CLASSES],name='Labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input for the network = Each row of the X_train matrix <br>\n",
    "Shape of weights of layer 1 = (shape of input of network, n_layer_1)<br>\n",
    "Input for layer 1 = (Input for the network $\\times$ weights_layer_1) + bias_layer_1<br>\n",
    "Output for layer 1 = relu(input for layer 1)<br>\n",
    "<br>\n",
    "Input for layer 2 = output of layer 1<br> \n",
    "Shape of weights of layer 2 = (shape of output of layer 1, n_layer_2)<br>\n",
    "Input of layer 2 = (Output of layer 1 $\\times$ weights_layer_2) + bias_layer_2<br>\n",
    "Output of layer 2 = relu(input for layer 2)<br>\n",
    "<br>\n",
    "Input for layer 3 = output of layer 2 <br>\n",
    "Shape of weights of layer 3 = (shape of output of layer 2, n_layer_3)<br>\n",
    "Input of layer 3 = (output of layer 2 $\\times$ weights_layer_3) + bias_layer_3<br>\n",
    "Output of layer 3 = softmax(input of layer 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input for any layer = (output for previous layer * weights for current layer) + bias for current layer<br>\n",
    "weights for any layer = trucnated normal distribution of shape (n_neurons in previous layer, n_neurons in current layer)<br>\n",
    "bias for any layer = an array of 0 with shape n_neurons in current layer<br>\n",
    "<br>\n",
    "If the layer is first layer, previous layer = input to the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_epochs = 50\r\n",
    "learning_rate = 1e-3\r\n",
    "\r\n",
    "n_hidden1 = 512\r\n",
    "n_hidden2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_layer(input, weight_dim, bias_dim, name):\r\n",
    "    \r\n",
    "    with tf.name_scope(name):\r\n",
    "        initial_w = tf.truncated_normal(shape=weight_dim, stddev=0.1, seed=42)\r\n",
    "        w = tf.Variable(initial_value=initial_w, name='W')\r\n",
    "\r\n",
    "        initial_b = tf.constant(value=0.0, shape=bias_dim)\r\n",
    "        b = tf.Variable(initial_value=initial_b, name='B')\r\n",
    "\r\n",
    "        layer_in = tf.matmul(input, w) + b\r\n",
    "        \r\n",
    "        if name=='out':\r\n",
    "            layer_out = tf.nn.softmax(layer_in)\r\n",
    "        else:\r\n",
    "            layer_out = tf.nn.relu(layer_in)\r\n",
    "        \r\n",
    "        tf.summary.histogram('weights',w)\r\n",
    "        tf.summary.histogram('biases',b)\r\n",
    "        return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model without dropout\r\n",
    "# layer_1 = setup_layer(input=X, weight_dim=[TOTAL_INPUTS,n_hidden1],bias_dim=[n_hidden1],name='layer_1')\r\n",
    "# layer_2 = setup_layer(input=layer_1, weight_dim=[n_hidden1, n_hidden2], bias_dim=[n_hidden2],name='layer_2')\r\n",
    "# layer_3 = setup_layer(input=layer_2, weight_dim=[n_hidden2, NR_CLASSES],bias_dim=[NR_CLASSES],name='out')\r\n",
    "# model_name = f'{n_hidden1}-{n_hidden2} LR {learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jain_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "layer_1 = setup_layer(input=X, weight_dim=[TOTAL_INPUTS,n_hidden1],bias_dim=[n_hidden1],name='layer_1')\r\n",
    "layer_drop = tf.nn.dropout(layer_1,keep_prob=0.8,name='dropout_layer')\r\n",
    "layer_2 = setup_layer(input=layer_drop, weight_dim=[n_hidden1, n_hidden2], bias_dim=[n_hidden2],name='layer_2')\r\n",
    "layer_3 = setup_layer(input=layer_2, weight_dim=[n_hidden2, NR_CLASSES],bias_dim=[NR_CLASSES],name='out')\r\n",
    "model_name = f'{n_hidden1}-{n_hidden2}-Dropout- LR {learning_rate} E{nr_epochs}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss, Optimization and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directory\n"
     ]
    }
   ],
   "source": [
    "#folder for tensorboard\r\n",
    "folder_name = f'{model_name} at {strftime(\"%H %M\")}'\r\n",
    "directory = os.path.join(LOGGING_PATH,folder_name)\r\n",
    "try:\r\n",
    "    os.makedirs(directory)\r\n",
    "except OSError as error:\r\n",
    "    print(error.strerror)\r\n",
    "else:\r\n",
    "    print('Successfully created directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jain_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('loss_calc'):\r\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=layer_3)) \r\n",
    "    #the actual value for comparison is Y and the predicted value is output of the network for calculatig losses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\r\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) #initialize the adam optimizer\r\n",
    "    train_step = optimizer.minimize(loss=loss) #use the optimizer to minimize loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('accuracy_calc'):\r\n",
    "    correct_pred = tf.equal(tf.argmax(layer_3,axis=1), tf.argmax(Y,axis=1)) #get the number of correct predictions by comparing y to yhat\r\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) #calculate the accuracy by taking mean of all epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('performance'):\r\n",
    "    tf.summary.scalar('accuracy',accuracy) #add a scalar to the summary by the name of accuracy which stores accuracy in it\r\n",
    "    tf.summary.scalar('Cost',loss) #add a scalar to the summary by the name of cost which stores the loss in it \r\n",
    "    # We add these scalars so that we can see them in tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check input images in tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('show_image'):\r\n",
    "    x_image = tf.reshape(X,[-1,IMAGE_HEIGHT,IMAGE_WIDTH,COLOR_CHANNELS])\r\n",
    "    tf.summary.image('Image_input',x_image,max_outputs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session() #create a tensorflow session "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup File Writer and merge summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_summary = tf.summary.merge_all() #create a merged summary for visualization \r\n",
    "train_writer = tf.summary.FileWriter(directory+'/train') #initialize a filewriter which dumps the data from the summary to the train folder \r\n",
    "train_writer.add_graph(sess.graph) #add the outputs from train writer to the tensorboard graph \r\n",
    "\r\n",
    "validation_writer = tf.summary.FileWriter(directory+'/validation') #create another file writer for validation dataset \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing all the variables \r\n",
    "init = tf.global_variables_initializer()\r\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_of_batch = 1000\n",
    "num_examples = y_train.shape[0]\n",
    "nr_iterations = int(num_examples/size_of_batch)\n",
    "\n",
    "index_in_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels): #create a function which helps us to break the data into batches of batch size \r\n",
    "    \r\n",
    "    global num_examples\r\n",
    "    global index_in_epoch\r\n",
    "\r\n",
    "    start = index_in_epoch\r\n",
    "    index_in_epoch += batch_size\r\n",
    "\r\n",
    "    if index_in_epoch>num_examples:\r\n",
    "        start = 0\r\n",
    "        index_in_epoch = batch_size\r\n",
    "\r\n",
    "    end = index_in_epoch\r\n",
    "    return data[start:end], labels[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t| Training Accuracy = 0.8500000238418579\n",
      "Epoch 1 \t| Training Accuracy = 0.8629999756813049\n",
      "Epoch 2 \t| Training Accuracy = 0.8669999837875366\n",
      "Epoch 3 \t| Training Accuracy = 0.8679999709129333\n",
      "Epoch 4 \t| Training Accuracy = 0.8700000047683716\n",
      "Epoch 5 \t| Training Accuracy = 0.8730000257492065\n",
      "Epoch 6 \t| Training Accuracy = 0.9750000238418579\n",
      "Epoch 7 \t| Training Accuracy = 0.9800000190734863\n",
      "Epoch 8 \t| Training Accuracy = 0.9800000190734863\n",
      "Epoch 9 \t| Training Accuracy = 0.984000027179718\n",
      "Epoch 10 \t| Training Accuracy = 0.9829999804496765\n",
      "Epoch 11 \t| Training Accuracy = 0.9869999885559082\n",
      "Epoch 12 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 13 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 14 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 15 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 16 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 17 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 18 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 19 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 20 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 21 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 22 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 23 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 24 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 25 \t| Training Accuracy = 0.9890000224113464\n",
      "Epoch 26 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 27 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 28 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 29 \t| Training Accuracy = 0.9900000095367432\n",
      "Epoch 30 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 31 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 32 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 33 \t| Training Accuracy = 0.9879999756813049\n",
      "Epoch 34 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 35 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 36 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 37 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 38 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 39 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 40 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 41 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 42 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 43 \t| Training Accuracy = 0.9919999837875366\n",
      "Epoch 44 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 45 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 46 \t| Training Accuracy = 0.9909999966621399\n",
      "Epoch 47 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 48 \t| Training Accuracy = 0.9929999709129333\n",
      "Epoch 49 \t| Training Accuracy = 0.9919999837875366\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nr_epochs): #train our network \r\n",
    "    #Training DataSet\r\n",
    "    for i in range(nr_iterations):\r\n",
    "        batch_x, batch_y = next_batch(batch_size=size_of_batch,data=x_train, labels=y_train) #get the x and y for the batches \r\n",
    "        feed_dictionary = {X:batch_x, Y:batch_y} #create a feed dictionary which contains data for training which has the placeholders holding our data\r\n",
    "        sess.run(train_step, feed_dict = feed_dictionary) #run the session by feeding the optimizer (train_step) and the data dictionary\r\n",
    "    s,batch_accuracy = sess.run(fetches=[merged_summary,accuracy], feed_dict=feed_dictionary) #obtain the output from the training epoch \r\n",
    "    #which fetches the summary and the accuracy from the epoch after training on the data \r\n",
    "    train_writer.add_summary(s,epoch) #use our training set file writer to add the summary to our disk after each epoch so we can visualize it on tensorboard\r\n",
    "    print(f'Epoch {epoch} \\t| Training Accuracy = {batch_accuracy}') #print the accuracy after each epoch \r\n",
    "    #Validation DataSet\r\n",
    "    summary = sess.run(fetches=merged_summary,feed_dict={X:x_val, Y:y_val}) #train the validation set in our neural network and fetch the summary from each epoch\r\n",
    "    #after training on validation dataset where the X placeholder stores x_val and Y placeholder stores y_val\r\n",
    "    validation_writer.add_summary(summary,epoch) #use the validation file writer to write the summary at each epoch to our disk so that we can visualize it on tensorboard\r\n",
    "print('Done Training') #print Done Training after the training has finished "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAYAAAByDd+UAAABAElEQVR4nO2Vyw2DMAyG3ap3RoAVmMCBrXICbwIbMEEEmzBCmMA9VFR9EGLTqhIV3xET/vjHjxMzM/yQ8y/FDsEgRAREtEnwJC2aWWAYhqfnzjmVoChDIroLISI45+5C2kwvsRemaYK+76HrOkiS5CmGiG8ZR2EB4zgGY0VRcF3Xks8wM7PI0jRNgzFEVCW4j7b4CLH5C3jv2RjD3nvxmWiVhphbJc/zt+pdY5NgWZYAcCuYqqp0hzUWWmvZGMPWWrX9KksfJ03TNKttEmNV8HWkqe1bIDi8iQjatoUsy8TNLbnQ6rbQDObZiej22Pz3F5DMVPE+/Bb/P0sPwf0LXgGAJwNqzP5nHgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=28x28 at 0x26319FEC5E0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('MNIST/test_img.png')\r\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = img.convert(mode='L')\r\n",
    "img_array = np.invert(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = img_array.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sess.run(fetches=tf.argmax(layer_3, axis=1),feed_dict={X:[test_img]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for test image is 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction for test image is {prediction[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 97.82%\n"
     ]
    }
   ],
   "source": [
    "accuracy_test = sess.run(fetches=accuracy,feed_dict={X:x_test, Y:y_test})\r\n",
    "print(f'The accuracy on the test set is {accuracy_test:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset for the next run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close all file writers and sessions and reset the graphs to release all resources\r\n",
    "train_writer.close() \r\n",
    "validation_writer.close()\r\n",
    "sess.close()\r\n",
    "tf.reset_default_graph()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "44d975d770f37f07d5939388c4dbb6ff4bc1df183a6a79e875df6871eed9264b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}