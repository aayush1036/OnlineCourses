{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit"
  },
  "interpreter": {
   "hash": "44d975d770f37f07d5939388c4dbb6ff4bc1df183a6a79e875df6871eed9264b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "source": [
    "# Constants"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2500\n",
    "\n",
    "TOKEN_SPAM_PROB_FILE = 'SpamData/03_Testing/prob-spam.txt'\n",
    "TOKEN_HAM_PROB_FILE = 'SpamData/03_Testing/prob-nonspam.txt'\n",
    "TOKEN_ALL_PROB_FILE = 'SpamData/03_Testing/prob-all-tokens.txt'\n",
    "\n",
    "TEST_FEATURE_MATRIX = 'SpamData/03_Testing/test-features.txt'\n",
    "TEST_TRAGET_FILE = 'SpamData/03_Testing/test-target.txt'"
   ]
  },
  {
   "source": [
    "# Load the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.loadtxt(TEST_FEATURE_MATRIX,delimiter=' ')\n",
    "y_test = np.loadtxt(TEST_TRAGET_FILE, delimiter=' ')\n",
    "prob_token_spam = np.loadtxt(TOKEN_SPAM_PROB_FILE, delimiter=' ')\n",
    "prob_token_ham = np.loadtxt(TOKEN_HAM_PROB_FILE, delimiter=' ')\n",
    "prob_all_tokkens = np.loadtxt(TOKEN_ALL_PROB_FILE, delimiter=' ')"
   ]
  },
  {
   "source": [
    "# Calculating Joint Probability"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The dimensions of the dot product between X_test and prob_token_spam are (1724,)\n"
     ]
    }
   ],
   "source": [
    "print(f'The dimensions of the dot product between X_test and prob_token_spam are {X_test.dot(prob_token_spam).shape}')"
   ]
  },
  {
   "source": [
    "## Set the priror \n",
    "\n",
    "$P(Spam|X) = \\frac{P(X|Spam)P(Spam)}{P(X)}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROB_SPAM = 0.3109"
   ]
  },
  {
   "source": [
    "```\n",
    "Taking log because having log reduces the calculation to addition and subtraction instead of multiplication and division \n",
    "and since the numbers are very close, taking log spreads them out so that the plots become much neater\n",
    "\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ -4.40757517,  -5.25364998,  -4.99005241, ..., -10.30243704,\n",
       "       -10.01475496, -12.0941965 ])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "np.log(prob_token_spam)"
   ]
  },
  {
   "source": [
    "# Joint probability in log format"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_log_spam = X_test.dot(np.log(prob_token_spam)-np.log(prob_all_tokkens)) + np.log(PROB_SPAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([24.28521714,  2.16166327, 20.5925235 , 17.76088562, 20.51597073])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "joint_log_spam[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_log_ham = X_test.dot(np.log(prob_token_ham)-np.log(prob_all_tokkens)) + np.log(1-PROB_SPAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-60.97491033, -11.01100812, -37.96946378, -59.14004125,\n",
       "       -53.79514888])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "joint_log_ham[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1724"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "joint_log_ham.size"
   ]
  },
  {
   "source": [
    "# Making Predictions \n",
    "\n",
    "## Checking for higher probabilities\n",
    "\n",
    "$P(Spam|X)>P(Ham|X)$\n",
    "\n",
    "$P(Spam|X)<P(Ham|X)$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "predictions = (joint_log_spam>joint_log_ham)*1\n",
    "predictions[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "y_test[-5:]"
   ]
  },
  {
   "source": [
    "# Metrics and Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_docs = (y_test == predictions).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Docs classified correctly 1685\n"
     ]
    }
   ],
   "source": [
    "print(f'Docs classified correctly {correct_docs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of documents classified incorrectly 39\n"
     ]
    }
   ],
   "source": [
    "num_docs_wrong = X_test.shape[0] - correct_docs\n",
    "print(f'Number of documents classified incorrectly {num_docs_wrong}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy is 97.738%\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct_docs/X_test.shape[0])*100\n",
    "print(f'Accuracy is {accuracy:.3f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fraction of emails misclassified 2.262%\n"
     ]
    }
   ],
   "source": [
    "print(f'Fraction of emails misclassified {100-accuracy:.3f}%')"
   ]
  },
  {
   "source": [
    "Demerits of accuracy <br>\n",
    "1.If there is a high number of positive outcomes and we build a model that classifies all the outcomes as positive then we have a bad model which can lead to serious problems but the accuracy will be high which is misleading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0, 1]), array([1136,  588], dtype=int64))"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "np.unique(predictions,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "569"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "true_pos = (y_test == 1)&(predictions == 1) #the email is spam and it is classified as spam\n",
    "true_pos.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "false_pos = (y_test == 0)&(predictions == 1) #the email is not spam but it is classified as spam\n",
    "false_pos.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "false_neg = (y_test == 1)&(predictions==0) #the email is spam but it is classified as not spam\n",
    "false_neg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1116"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "true_neg = (y_test==0)&(predictions==0) #the email is not spam but it is classified as not spam\n",
    "true_neg.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 569,   19],\n",
       "       [  20, 1116]])"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "confusion_matrix = np.array([[true_pos.sum(), false_pos.sum()],[false_neg.sum(), true_neg.sum()]])\n",
    "confusion_matrix"
   ]
  },
  {
   "source": [
    "$Recall = \\frac{TP}{TP+FN}$ \n",
    "\n",
    "We can think of this as out of all the spam emails, how many spam emails did we classify correctly\n",
    "\n",
    "Weakness of recall score is that it can be easily maximized/manipulated by labelling all emails as spam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = true_pos.sum()/(true_pos.sum()+false_neg.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The recall score is 0.966\n"
     ]
    }
   ],
   "source": [
    "print(f'The recall score is {recall:.3f}')"
   ]
  },
  {
   "source": [
    "$Precision = \\frac{TP}{TP+FP}$\n",
    "\n",
    "Ratio of correctly classified spam messages to the total number of times we predicted spam"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The precision is 0.968\n"
     ]
    }
   ],
   "source": [
    "precision = true_pos.sum()/(true_pos.sum()+false_pos.sum())\n",
    "print(f'The precision is {precision:.3f}')"
   ]
  },
  {
   "source": [
    "F Score = $2 \\times \\frac{precision \\times recall}{precision + recall}$\n",
    "\n",
    "F score is the harmonic mean of precision and recall and because of this it takes both the false positives and the false negatives into account and also it has a value between 0 and 1 so it provides a universal scale for comparison"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The F1 score is 0.967\n"
     ]
    }
   ],
   "source": [
    "f1_score = 2*((precision*recall)/(precision+recall))\n",
    "print(f'The F1 score is {f1_score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}